---
title: "Coding Assignment 3"
author: "Team 06"
date: "Due: 2022-12-09 23:59"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
library(readxl)
library(dplyr)
library(gt)
library(gtsummary)
library(car) # for vif function
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 50 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan. 

The data on the 50 customers in the sample is as follows:

-	Charges: Total medical expenses for a particular insurance plan (in dollars)
-	Age: Age of the primary beneficiary
-	BMI: Primary beneficiary’s body mass index (kg/m2)
-	Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-	Children: Number of children covered by health insurance plan (includes other dependents as well)
-	Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-	Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.


```{r}
insurance <- read.csv("../Data/insurance_0126_Group6.csv")

gt(head(insurance))
```



## Question 1

Randomly select three observations from the sample and exclude from all modeling (i.e. n=47). Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 47 observations.

```{r q1}
set.seed(123)

index <- sample(seq_len(nrow(insurance)), size = 3)

train <- insurance[-index,]
test <- insurance[index,]
summary(train)

train %>%
  tbl_summary(statistic = list(all_continuous() ~ c("{mean} ({sd})",
                                      "{median} ({p25}, {p75})",
                                      "{min}, {max}"),
                all_categorical() ~ "{n} / {N} ({p}%)"),
type = all_continuous() ~ "continuous2")
```
According to the summary statictic output, the dependent variable,Charges is skewed to the right since it mean is higher than the median by about 2 thousand dollars. Since the standard deviation is high, some outliers could exist in the data.

## Question 2

Provide the correlation between all quantitative variables

```{r}
cor(insurance)

```
According to the correlation output,"Age" and "Smoker" variables are strongly and directly related to the dependent variable,Charges.

## Question 3

Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?


```{r fig.height=8, fig.width=8}
original_model <- lm(Charges ~., data = train[,c(1:9)] ) #pulling only original data column before data transformation

summary(original_model)

par(mfrow=c(2,2))
plot(original_model)
vif(original_model)
```
## Question 3 Answers=>
No significant VIFs are found for independent variables and no concerning multicolllinearity is detected.

The "Residual vs Fitted" model tests for the third Gauss-Markov  assumption. The model shows that our results are non-linear as the data curves down and then shoots up. If it were properly specified, it would be a perfectly horizontal line with a bit of noise of around it. This is a violation of the third assumption. The solution would be to consider using ratios or percentages rather than raw data.

The "Normal Q-Q" model" checks the sixth Gauss-Markov assumption, which is that we have a normally distributed error term. In this case, there is a violation of the sixth assumption because there are large variables going far outside of our line (top right corner). To correct this violation we can fit the models to several sets of transformed data and then check to see which transformation appears to produce the most normally distributed residuals.

The "Scale-Location" model tests for heteroscedasticity. Our results show that there is a violation of the fourth Gauss-Markov assumption as the data is not in a horizontal line with corresponding noise; rather as the size gets larger, our data is spread is widening creating a "fanning out" effect. To correct this violation, we should look for subgroups in data and analyze separately. We should also use summary data (like the mean value) rather than the raw data.

The "Residuals vs Leverage" model checks for outliers. There are no Gauss-Markov violations shown.

## Question 4

Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data and run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?

```{r q4}

#to detect non-linear realtionships and non-normally distributed variables
scatterplotMatrix(train[,c(1,2,3,5)])
```
Some nonlinearity between Charges and Age have been found.

```{r q4}
#Data Transformation
#taking natural logirthm of Charges to bring possible outliers closer

par(mfrow=c(1,2))

hist(train$Charges) #before

train$lnCharges <- log(train$Charges)

hist(train$lnCharges) #after

```
Question#4 Answer==>
Taking natural logirthm of Charges transfrom the dependent variable to be more normally distributed.
 

```{r q4}
#Plotting the relationships After Transformation
scatterplotMatrix(train[,c(10,2,3)]) # grabbing lnCharges

#to determine if the relationship between Charges and Age is logarithmic or quadratic.

train$lnAge <- log(train$Age)
train$AgeSquared <- train$Age^2
scatterplotMatrix(train[,c(10,11,3)]) # lnChares & lnAge
scatterplotMatrix(train[,c(10,12,3)]) # lnChares & AgeSquared

```



```{r fig.height=8, fig.width=8}
# New Regression
# Models to look at the regression coefficients.

# Model 1lnAge: Age with a logaritmic shape

model_1lnAge <- lm(lnCharges ~., data = train[,c(10,11,3:9)] )

summary(model_1lnAge)

# or

tbl_regression(model_1lnAge,
               estimate_fun =  ~style_sigfig(.x, digits = 4)) %>% as_gt() %>%
  gt::tab_source_note(gt::md(paste0("Adjusted R-Squared: ",round(summary(model_1lnAge)$adj.r.squared* 100,digits = 2),"%")))
par(mfrow=c(2,2))
plot(model_1lnAge)
```



```{r fig.height=8, fig.width=8}
# Model 2AgeSq:Quadratic Relationship between Age and Charges

model_2AgeSq <- lm(lnCharges ~., data = train[,c(10,3:9,12)] )

summary(model_2AgeSq)

# or

tbl_regression(model_2AgeSq,
               estimate_fun =  ~style_sigfig(.x, digits = 4)) %>% as_gt() %>%
  gt::tab_source_note(gt::md(paste0("Adjusted R-Squared: ",round(summary(model_2AgeSq)$adj.r.squared* 100,digits = 2),"%")))
par(mfrow=c(2,2))
plot(model_2AgeSq)

```

```{r fig.height=8, fig.width=8}
# # Model 3lnBMI: BMI with a logaritmic shape
train$lnBMI <- log(train$BMI)

model_3lnBMI <- lm(lnCharges ~., data = train[,c(10,11,4:9,13)] )

summary(model_3lnBMI)

# or

tbl_regression(model_3lnBMI,
               estimate_fun =  ~style_sigfig(.x, digits = 4)) %>% as_gt() %>%
  gt::tab_source_note(gt::md(paste0("Adjusted R-Squared: ",round(summary(model_3lnBMI)$adj.r.squared* 100,digits = 2),"%")))
par(mfrow=c(2,2))
plot(model_3lnBMI)
```
## Question 4 Answer=> 
After the data transformation on both dependent variable, Charges and 2 independent variables, Age and BMI, regression Model3 shows improved measures. Residual Standard Error has decreased to 0.2518. Adjusted R^2 has increased to 92%. F statistic is also showing a higher number of 68.75 compared to that of Original model.The signs of coefficient didn't change. Additionally, the independent variable "Children" has become statistically significant for Model3 which was not significant for Original Model.

## Question 5

Use the 3 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

```{r q5}
test$lnCharges <- log(test$Charges)
test$lnAge <- log(test$Age)
test$AgeSquared <- test$Age^2
test$lnBMI <- log(test$BMI)

test$original_model_pred <- predict(original_model, newdata = test)

test$model_1lnAge_pred <- predict(model_1lnAge,newdata = test) %>% exp()

test$model_2AgeSq_pred <- predict(model_2AgeSq,newdata = test) %>% exp()

test$model_3lnBMI_pred <- predict(model_3lnBMI,newdata = test) %>% exp()

```

```{r q5}
# Finding the error

test$error_OM <- test$original_model_pred - test$Charges

test$error_1lnAge<- test$model_1lnAge_pred - test$Charges

test$error_2SqAge<- test$model_2AgeSq_pred - test$Charges

test$error_3lnBMI<- test$model_3lnBMI_pred - test$Charges

```

```{r q5}
# Bias

# Original Model
mean(test$error_OM)

# Model 1lnAge
mean(test$error_1lnAge)

# Model 2SqAge
mean(test$error_2SqAge)

#Model 3lnBMI
mean(test$error_3lnBMI)

```
BIAS
[1] 4879.738
[1] 2656.491 
[1] 7168.83
[1] 2627.923 (best) 
```{r q5}
# MAE

mae <- function(error_vector){
  error_vector %>% 
  abs() %>% 
  mean()}

# Original Model
mae(test$error_OM)

# Model 1lnAge
mae(test$error_1lnAge)

# Model 2SqAge
mae(test$error_2Sq)

#Model 3lnBMI
mae(test$error_3lnBMI)

```
MAE
[1] 5721.517
[1] 9328.276 
[1] 12033.55
[1] 9300.512 better
```{r q5}
### RMSE
rmse <- function(error_vector){
   error_vector^2 %>% 
  mean() %>% 
  sqrt()}

# Original Model
rmse(test$error_OM)
# Model 1lnAge
rmse(test$error_1lnAge)

# Model 2SqAge
rmse(test$error_2SqAge)

#Model 3lnBMI
rmse(test$error_3lnBMI)

```
RMSE
[1] 6627.591
[1] 11853.78 better
[1] 17117.81
[1] 11808.4

```{r}
### MAPE
mape <- function(error_vector, actual_vector){
  (error_vector/actual_vector) %>% 
    abs() %>% 
    mean()
}

# Original Model
mape(test$error_OM, test$Charges)
# Model 1lnAge
mape(test$error_1lnAge, test$Charges)
# Model 2SqAge
mape(test$error_2SqAge, test$Charges)
# Model 3lnBMI
mape(test$error_3lnBMI, test$Charges)

```
# Question 5 Answer
Summary of Performance Metrics
BIAS
[1] 4879.738
[1] 2656.491 lower bias better for long run
[1] 7168.83
[1] 2627.923 (best) over prediction
MAE
[1] 5721.517
[1] 9328.276 
[1] 12033.55
[1] 9300.512 better
RMSE 
[1] 6627.591
[1] 11853.78 
[1] 17117.81
[1] 11808.4 better
MAPE
[1] 0.3584357
[1] 0.406728
[1] 0.4912408
[1] 0.4066504 better

is our interest long run or short run? short run
which model is the best? =>Model3lnBMI

The original model is bad model with high BIAS value and High RMSE value and. Comparing the proposed models with transformed data, Model3lnBMI has better BIAS and RMSE value.Model3lnBMI has higher MAE value than that of the Original Model due to possible outliers.However,after normalizing the errors, the MAPE value of Model3lnBMI became almost the same as the original MAPE. With Lowest RMSE among the models and lowest BIAS value, Model3lnBMI is the best model for the short run.

## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.

```{r}
### Marginal Change Analysis

model_3lnBMI$coefficients["lnAge"]
model_3lnBMI$coefficients["Female"]
model_3lnBMI$coefficients["Children"]
model_3lnBMI$coefficients["Smoker"]
model_3lnBMI$coefficients["WinterSprings"]

```
# Question 6 Answer==>
 Marginal Change Analysis
 lnAge 
1.53895 
   Female 
0.1360697 
  Children 
0.07963402 
  Smoker 
1.255853 
WinterSprings 
0.1992206 

According to the Model3 coefficients, Age is the strongest determinant for the premium charges and smoker found to be the second in order. Coefficient sings also show the true relationship between each independent variable and the charges.
lnAge, Female, Children and Smoker and Winter Springs all tested statistically significant.WinterPark, Oviedo, and LnMBI did not test statistically significant so we are unable to change perform marginal change analysis for these variables.
Our coefficient for lnAge represents that a 1% change will affect charges by 1.5%.
Our coefficient for Female represents that a 1% change will affect charges by 1.5%.
Our coefficient for Children represents that a 1% change will affect charges by .08%.Our coefficient for Winter Springs represents that a 1% change will affect charges by .2%.The coefficients are consistent with what we expected regarding their sign. For example, we assumed that an increase in age, an increase in BMI, and being a smoke would increase charges. Also women tend to incur greater health costs than men, particularly during the reproductive years, so the positive sign is also consistent with what we expect.

## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
| -------- | --- | --- | ------ | -------- | ------ | -------------- | 
| 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
| 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
| 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
| 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
| 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |
```{r}
PredictionCUS1 <- data.frame(Age = 60,
                            lnAge=4.094345 ,
                            BMI = 22,
                            lnBMI=3.091042,
                            Female = 1,
                            Children=0,
                            Smoker=0,
                            city=0,
                            WinterSprings=0,
                            WinterPark=0,
                            Oviedo=1)

predict(model_3lnBMI, newdata =PredictionCUS1,interval = "confidence",
      level = .5)
C1<-predict(model_3lnBMI, newdata =PredictionCUS1,interval = "confidence",
      level = .5)

# calculate the inverse log using exp( ) function. 
antilog_C1 = exp (C1)
print(antilog_C1)
```

```{r}
PredictionCUS2 <- data.frame(Age = 40,
                            lnAge=3.688879 ,
                            BMI = 30,
                            lnBMI=3.401197,
                            Female = 0,
                            Children=1,
                            Smoker=0,
                            city=1,
                            WinterSprings=0,
                            WinterPark=0,
                            Oviedo=0)

predict(model_3lnBMI, newdata =PredictionCUS2,interval = "confidence",
        level = .5)
C2<-predict(model_3lnBMI, newdata =PredictionCUS2,interval = "confidence",
      level = .5)
antilog_C2 = exp (C2)
print(antilog_C2)
```

```{r}
PredictionCUS3 <- data.frame(Age = 25,
                            lnAge=3.218876 ,
                            BMI = 25,
                            lnBMI=3.218876,
                            Female = 0,
                            Children=0,
                            Smoker=1,
                            city=0,
                            WinterSprings=0,
                            WinterPark=1,
                            Oviedo=0)

predict(model_3lnBMI, newdata =PredictionCUS3,interval = "confidence",
        level = .5)
C3<-predict(model_3lnBMI, newdata =PredictionCUS3,interval = "confidence",
      level = .5)
antilog_C3 = exp (C3)
print(antilog_C3)
```
```{r}
PredictionCUS4 <- data.frame(Age = 33,
                            lnAge=3.496508 ,
                            BMI = 35,
                            lnBMI=3.555348,
                            Female = 1,
                            Children=2,
                            Smoker=0,
                            city=0,
                            WinterSprings=1,
                            WinterPark=0,
                            Oviedo=0)

predict(model_3lnBMI, newdata =PredictionCUS4,interval = "confidence",
        level = .5)
C4<-predict(model_3lnBMI, newdata =PredictionCUS4,interval = "confidence",
      level = .5)
antilog_C4 = exp (C4)
print(antilog_C4)
```

```{r}
PredictionCUS5 <- data.frame(Age = 45,
                            lnAge=3.806662 ,
                            BMI = 27,
                            lnBMI=3.295837,
                            Female = 1,
                            Children=3,
                            Smoker=0,
                            city=0,
                            WinterSprings=0,
                            WinterPark=0,
                            Oviedo=1)

predict(model_3lnBMI, newdata =PredictionCUS5,interval = "confidence",
        level = .5)
C5<-predict(model_3lnBMI, newdata =PredictionCUS5,interval = "confidence",
      level = .5)
antilog_C5 = exp (C5)
print(antilog_C5)
```
# Question 7 Answer=>
Prediction Intervals
Customer1:  fit      lwr      upr
          11945.47 10981.95 12993.52
Customer2:  fit      lwr      upr
          6177.858 5822.709 6554.668
Customer3:  fit     lwr      upr
          8744.609 7898.18 9681.747
Customer4:  fit     lwr      upr
          6974.123 6572.32 7400.491
Customer5:  fit      lwr      upr
          9773.352 9095.658 10501.54

## Question 8

The owner notices that some of the predictions are wider than others, explain why.

Using individual observances instead of the mean increases uncertainty and results in a wider range of possible values.
The more spread the data is, the larger the variance is in relation to the mean. This can cause a prediction interval with a wider range of values.
Extrapolation error could also cause the wider prediction.

## Question 9 

Are there any prediction problems that occur with the five potential clients? If so, explain.

Model3lnBMI's high MAPE value can signal some potential problems with prediction accuracy for the potential clients. However,Model3's high MAPE value can be due to the frequent changes in premium charges or the " zero" values in the data set. Since there is no industry standard for a good MAPE value, the model's high MAPE can be ignored. For instance, customer1 is predicted to be charged the most for his/age despite his/her low BMI while customer2 is being charged the least for being a non smoker in his middle age with higher BMI. Since the charges reflect the predictors well,
there is no concerning prediction problems.